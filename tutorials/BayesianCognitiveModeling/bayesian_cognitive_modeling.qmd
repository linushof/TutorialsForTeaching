---
title: "Fitting Cognitive Models"
subtitle: 'Student Tutorial'
authors: "Linus Hof, Nuno Busch"
date: '`r Sys.time()`'
format: html
---


```{r}
# packages 
pacman::p_load(readxl , 
               tidyverse , 
               magrittr , 
               knitr
               )
```



## Risky Choice


## Expected Values


## Cumulative Prospect Theory

$$
V = \sum_i^n v(x_i) \pi_i \; 
$$

$$
v(x_i) = \begin{cases} 
x_i^\alpha & x_i \geq 0 \; ,\\
-\lambda |x_i|^\alpha & else
\end{cases}
$$
```{r}
vf <- expand_grid(outcome = c(seq(-10, 10, .1)) , 
                  alpha = seq(.8,.9,.1)) %>% 
  mutate(v = case_when(outcome >= 0 ~ outcome^alpha , 
                       outcome < 0 ~ -1*(abs(outcome)^alpha) 
                       )
         ) 

vf %>% ggplot(aes(outcome, v, group = alpha, color=alpha)) + 
  geom_hline(yintercept = 0) + 
  geom_vline(xintercept = 0) + 
  geom_abline(slope=1, linetype='dashed', linewidth = 1, color = 'gray') + 
  geom_line(linewidth = 1) + 
  theme_minimal()
```


$$
\pi_i = \begin{cases}
... \\
...
\end{cases}
$$

$$
w(p_i) = \frac{\delta p_i^\gamma}{\delta p_i^\gamma + (1-p_i)^\gamma}
$$

```{r}
wf <- expand_grid(p = c(seq(0, 1, .01)) , 
                  gamma = c(.5, .8) , 
                  delta = c(.5, 1, 1.5)) %>%
  mutate(w = ( delta*(p^gamma) ) / (  (delta*(p^gamma)) + (1-p)^gamma ) )


wf %>% ggplot(aes(p, w, group = gamma, color = gamma)) +
  facet_wrap(~delta, nrow=1) + 
  geom_abline(slope=1, linetype='dashed', linewidth = 1, color = 'gray') +
  geom_line(linewidth = 1) + 
  theme_minimal()
```


```{r}
# example prediction cpt 
```


## Bayesian Model Fitting

Similar to linear regression etc. the goal is to find the combination of parameters 


$$
p(\Theta) \to p(\Theta|D)
$$

$$
p(\Theta|D) \propto p(\Theta) p(D|\Theta)
$$ 

When $\Theta = \{\alpha, \gamma, \delta\}$, we can also write

$$
p(\alpha, \gamma, \delta|D) \propto p(\alpha, \gamma, \delta) p(D|\alpha, \gamma, \delta)
$$

The idea is to compute the probability terms on the right with different combinations for the parameters $\alpha, \gamma, \delta$.
The different posterior probabilities (joint posterior distribution) can be compared to assess the support for the different parameter values given our prior knowledge and the new data.

The prior is the probability distribution over possible parameter values.

## Likelihood

The likelihood is the probability of the data given the model.
For instance, in the case of a binary choice between two monetary gambles, one could have a model, that always predicts 1.

```{r}
rbinom(100,1,prob=1)
```

Given the model, the observation that option 0 was chosen would be 0.

```{r}
dbinom(1,1,prob = 1)
```

Coming back to our CPT model, a deterministic version of CPT would make the computations according to equations above and then predict that the option with the higher evaluation is always chosen.

In a stochastic version, the probability of choosing the option with a higher valuations depends on the magnitude of the differences In this case, the predicted probability for each choice can take values different from 0 and 1.

...

Assuming that the choices are independent from each other, we obtain the likelihood for the whole data by simply multiplying the predicted probability for each choice.

Integrating the calculation across the entire set of parameter values gives the likelihood distribution.

# Using MCMC to obtaint the posterior

Since these integrals are often hard to solve (no analytic solution), Bayesian inference relies on numerical approximation procedures.
MCMC is the current standard for approximating the posterior.
The idea is ...

Applying MCMC returns the entire posterior distribution, which can be communicated in multiple ways.
Below, we look at an implementation JAGS
