---
title: "Bayesian Cognitive Modeling"
subtitle: "Tutorial"
author: "Linus Hof & Nuno Busch"
date: "`r Sys.time()`"
output:
  html_document:
    keep_md: true
---

```{r include=FALSE}
# packages 
pacman::p_load(readxl , 
               tidyverse , 
               magrittr , 
               knitr, 
               here , 
               R2jags
               )

set.seed(1) # setting seed to make random samples reproducible
```

```{r include=FALSE}
here::i_am("BayesianCognitiveModeling/bayesian_cognitive_modeling.Rmd")
```


## Risky Choice

Sometimes in your life, you make choices between certain outcomes (e.g., whether to buy an Android phone or an iPhone). In other situations, you have to make risky choices:
When making decisions under risk, choosing an option does not always lead to the same outcome.
Instead, choosing an option can lead to one of several outcomes with some probability. A classical example would be playing a lottery or to invest in the stock market, another real life example would be deciding to buy insurance or not (where the future event of getting a disease is not for sure but may only happen with some predicted probability).

In our studies, we often investigate people's choice behavior in a controlled context. For this, we present them with different lotteries ("risky prospects") and ask which one they prefer.
Consider the following risky choice problem from a study of [Kellen et al. (2016)](https://linkinghub.elsevier.com/retrieve/pii/S0010027716302104), involving two options, $A$ and $B$. 
The different columns represent different features of both options.

```{r}
#| echo: false
#| output: false

# read data
problems <- read_xlsx(here("BayesianCognitiveModeling", "kellen2016.xlsx"), sheet = 1) # problem features
description <- read_xlsx(here("BayesianCognitiveModeling", "kellen2016.xlsx"), sheet = 3, col_names = TRUE) # DFD choices 

# clean data

problems %<>% 
  select(-c(`...7`,`...8`,`...13`,`...14`)) %>% 
  mutate(domain = case_when(outcomeA1 >= 0 & outcomeA2 >= 0 & outcomeB1 >= 0 & outcomeB2 >= 0 ~ "Gain" , 
                            outcomeA1 <= 0 & outcomeA2 <= 0 & outcomeB1 <= 0 & outcomeB2 <= 0 ~ "Loss" , 
                            .default = "Mixed"))  %>% 
  select( problem, label, domain, everything()) %>% 
  arrange(domain)

description  %<>% rename(label = ...1)
```



```{r}
#| eval: false

# read data
problems <- read_xlsx("BayesianCognitiveModeling/kellen2016.xlsx", sheet=1) # problem features

# clean data
problems %<>% 
  select(-c(`...7`,`...8`,`...13`,`...14`)) %>% 
  mutate(domain = case_when(outcomeA1 >= 0 & outcomeA2 >= 0 & outcomeB1 >= 0 & outcomeB2 >= 0 ~ "Gain" , 
                            outcomeA1 <= 0 & outcomeA2 <= 0 & outcomeB1 <= 0 & outcomeB2 <= 0 ~ "Loss" , 
                            .default = "Mixed"))  %>% 
  select( problem, label, domain, everything()) %>% 
  arrange(domain)
```


```{r}
kable(problems[1,])
```

For instance, `Option B` has two possible outcomes, indicated by `outcomeB1` and `outcomeB2`. 
When choosing this option, one of the outcomes is obtained.
Specifically, `outcomeB1`=`r problems[[1,'outcomeB1']]` occurs with `probB1`=`r problems[[1,'probB1']]` and `outcomeB2`=`r problems[[1,'outcomeB2']]` occurs with `probB2`=`r problems[[1,'probB2']]`. 
When choosing this option repeatedly, we therefore expect to obtain `4` most of the time, but sometimes also a `0`.
To illustrate, we simulate choosing `Option B` 100 times. 

```{r}
sample(c(problems[[1,'outcomeB1']], problems[[1,'outcomeB2']]) , 
       size=100 ,
       replace = TRUE ,
       prob=c(problems[[1,'probB1']], problems[[1,'probB2']]))
```

Turning to `Option A`, we notice that `outcomeA1`=`r problems[[1,'outcomeA1']]` occurs with `probA1`=`r problems[[1,'probA1']]` and `outcomeA2`=`r problems[[1,'outcomeA2']]` occurs with `probA2`=`r problems[[1,'probA2']]`.
Thus, this option is effectively a safe option, where `outcomeA2` does not really exist and we instead always obtain a `3`. 
(Note that we still need to keep the (in reality non-existent) outcome A2 in our dataframe for modeling purposes. We will get to that later.). 

To illustrate, we also simulate choosing `Option A` 100 times. 



```{r}
sample(c(problems[[1,'outcomeA1']], problems[[1,'outcomeA2']]) , 
       size=100 ,
       replace = TRUE ,
       prob=c(problems[[1,'probA1']], problems[[1,'probA2']]))
```

In a typical risky choice study, people are presented with the possible outcomes and probabilities of both options and need to decide which of the options they prefer (so-called *decisions from description*). 
However, most studies involve more than just one choice problem. 
For instance, in the study by Kellen et al. (2016), participants were presented with `r nrow(problems)` different choice problems, each represented by a row in the table below. 
Each problem is a unique combination of the outcomes and probabilities for `Option A` and `Option B`.

The column `domain` indicates whether all possible outcomes are $\geq 0$ (`Gain`), or whether all outcome are $\leq 0$ (`Loss`), or whether the problem involves some outcomes that are $>0$ and some that are a $<0$ - i.e., a problem featuring a gamble that could lead to either a gain, or a loss. 

```{r}
kable(problems)
```


## Expected Value Maximization

What do you think, how do people choose between such lotteries? They are probably trying to maximize rewards (gain the highest outcome values and avoid losing any) on the long run, right?

This assumption is reflected in an influential theory: According to this view, the rational way to make such decisions under risk is to choose the option with the highest *expected value* (EV), i.e., 

$$
\arg\max  EV = \sum_i^np_ix_i \; ,
$$
where $x_i$ are the possible outcome of the option and $p_i$ are their respective probabilities.
That is, for each option, people should first multiple all outcomes by their probabilities and then sum up the products to obtain the EV.
Then they should choose the option with the higher EV.
The following code follows this procedure for all choice problems used by Kellen et al. (2016).
Below we showcase the choice predicted by EV maximization for the first five gambles of the dataframe, where the last column `max_ev_det` refers to the lottery with the higher EV (`Option A` = 1, `Option B` = 0) - i.e., the predicted choice.


```{r}
problems %<>%  
  mutate(ev_A = round( probA1*outcomeA1 + probA2*outcomeA2 , 2) , # computes EV of option A and rounds the result to 2 decimal places
         ev_B = round( probB1*outcomeB1 + probB2*outcomeB2 , 2) , # computes EV of option B and rounds the result to 2 decimal places
         # choose option A (coded as 0) when ev_A > ev_B, otherwise B (coded as 1): nothing when ev_A = ev_B
         max_ev_det = case_when(ev_A > ev_B ~ 1 , # choose option A when ev_A > b
                                ev_A < ev_B ~ 0 , # choose option B when ev_A < b
                                ev_A == ev_B ~ NA ) # choose nothing when EV's are equal (NA refers to missing values that are created here)
         )
kable(head(problems))
```

One of the shortcomings of the idea of EV maximization is that people do not seem to maximize expected values in real life.  
In their study, Kellen et al. (2016) collected the choices of `r ncol(description)-1` participants on all of the `r nrow(description)` choice problems from above.
To illustrate, the following table shows the choices of 10 participants (columns) on the first 10 choice problems (rows). Here, again, values of 1 refer to a choice of `Option A`, and values of 0 mean that `Option B` was chosen. 


```{r}
#| echo: false
#| output: false

# read and clean data
description <- read_xlsx(here("BayesianCognitiveModeling", "kellen2016.xlsx"), sheet = 3, col_names = TRUE)
description  %<>% rename(label = ...1)
```

```{r}
#| eval: false

# read and clean data
description <- read_xlsx("BayesianCognitiveModeling/kellen2016.xlsx", sheet = 3, col_names = TRUE)
description  %<>% rename(label = ...1)
```

```{r}
kable(description[1:10, 1:11])
```

Since we know for all problems whether `Option A` (`1`) or `Option B` (`0`) has the higher EV, we can compute the proportion of choices, in which participants chose the option with the higher EV. 

The last column of each row, `ev_max_ob`, shows the proportion of people who choose the option with the higher EV in the respective problem.
It is easy to see that people often deviate from strict EV maximization, sometimes very systematically.


```{r}
problems <- problems %>%   
  left_join(description, by = join_by(label)) %>% # here, we merge our choice data with the dataframe containing the problem descriptions and the calculated EV and choice predictions.
  mutate(ev_max_obs = round ( case_when(max_ev_det == 1 ~ rowMeans(select(., `1_er171989`:`99_AA211989`), na.rm = TRUE)  , # this command computes the proportion of participants that maximized EV in a particular choice problem.
                                        max_ev_det == 0 ~ 1 - rowMeans(select(., `1_er171989`:`99_AA211989`), na.rm = TRUE) 
                                        ) , 2 # rounding to two decimal places
                              )
         ) 

problems %>% 
  select(problem:max_ev_det, ev_max_obs) %>%
  kable()
```



## Cumulative Prospect Theory: A Cognitive Model for Risky Choice

One of the key accomplishments in decision making research is the development of cumulative prospect theory (CPT) by Amos Tversky and Daniel Kahneman, for which Kahneman received the Nobel prize in 2002. Prospect theory aims to describe what choices people are actually making, if not maximizing the EV. 

CPT retains the fundamental idea of EV maximization that people weight the outcomes of each option by their probability. 
However, a core innovation is the assumption that people do not treat the outcomes and probabilities objectively as they are, but instead represent them subjectively in a way that they become distorted. 

We will explain the formalities in detail below, but the main idea may be illustrated with simplifying examples:  
**Subjective utilities:** Outcomes may not have the same subjective value for everyone, dependent on a person's current financial status (the reference point), and desirability may also change depending on the outcome. For example, 10 EUR may feel subjectively more valuable to you when you get them as a bonus on top of 5 EUR (total of 15 EUR), than when you get them as bonus on top of 100 EUR (= total of 110). This is called diminishing marginal utility.  
**Loss aversion**: The concept of loss aversion complements that of subjective utilities by the observation that "losses loom larger than gains": For example, most people would prefer sticking with the status quo instead of playing when offered a toin coss with equal probility of winning 120 EUR or losing 100 EUR, despite positive expected value. Consequently losses are given more weight than equally sized gains.
**Subjective decision weights:** Individuals may behave as if some events are more or less likely than they actually are. For example, people typically attribute excessive weight to events with low probability and insufficient weight to events with high probability (Do you think this could explain why people go to casinos?).

In other words, prospect theory assumes that people choose the option that yields the higher expected utility for them, where the expected utility of an option is determined by summing up the subjective utility of each outcome weighted by its decision weight.

Mathematically, these assumptions are expressed by replacing the outcome values $x_i$ with subjective values (subjective utilities) $v(x_i)$ and replacing the probabilities with subjective decision weight $\pi_i$.

$$
\arg\max_x V = \sum_i^n v(x_i) \pi_i \; ,
$$

where $v(.)$ is a so-called value function that creates subjective utilities by distorting objective outcome values. $\pi_i$ on the other hand is obtained by transforming the objective probabilities into decision weights with a probability weighting function $w(.)$.

Next, we consider the value and the probability weighting function in more detail.

### Value function

The value function takes each objective outcome $x_i$ as input and returns a respective subjective value $v(x_i)$ as output, according to:

$$
v(x_i) = \begin{cases} 
x_i^\alpha & x_i \geq 0 \; ,\\
-\lambda |x_i|^\alpha & x_i <0 \
\end{cases}
$$
where $\alpha$ is a number that usually takes values between $0$ and $1$, and $\lambda$ is a number typically $>0$. Both $\alpha$ and $\lambda$ are called *free parameters* because they can take varying values which can be estimated: For example, given choice data from a certain participant, we can estimate what parameters fit best to their value function to best explain the why they chose what they chose (we'll get to that later).  

$\alpha$ controls the curvature of the value function (the diminishing marginal utility), $\lambda$ controls the steepness of the value function in the loss domain relative to the gain domain (loss aversion).
The figure below illustrates the form of this value transformation when $\lambda = 1.2$, for different values of alpha: $\alpha = 1$, $\alpha = .9$, $\alpha = .8$.

```{r value-function-demo}
vf <- expand_grid(outcome = c(seq(-10, 10, .1)) , 
                  alpha = seq(from = .8, to = 1, by = .1)) %>% 
  mutate(v = case_when(outcome >= 0 ~ outcome^alpha , 
                       outcome < 0 ~ -1.2*(abs(outcome)^alpha) 
                       )
         ) 

vf %>% ggplot(aes(outcome, v, group = as.factor(alpha), color=as.factor(alpha))) + 
  geom_hline(yintercept = 0) + 
  geom_vline(xintercept = 0) + 
  geom_abline(slope=1, linetype='dashed', linewidth = 1, color = 'gray') + 
  geom_line(linewidth = 1) + 
  theme_minimal()
```

It can be seen that in the case of $\alpha = 1$ (blue line), all subjective values $v(x)$ are equal the objective outcomes $x$ (except for $x_i < 0$: because of $\lambda > 1$. 
The smaller $\alpha$ gets, the more curved (concave) is the line, indicating that the same change in objective outcomes (on the x-axis) leads to a smaller change in the subjective values (on the y-axis), the more extreme (more positive/negative) the objective outcomes get.
This is the most common form observed in empirical data when fitting the $\alpha$ parameter to the observed choices (see below).  

### Probability weighting function

The weighting function takes a probability $p_i$ between $0$ and $1$ as input and returns a *transformed* probability (a decision weight) $w(p_i)$ as output, according to:

$$
w(p_i) = \frac{\delta p_i^\gamma}{\delta p_i^\gamma + (1-p_i)^\gamma} \; ,
$$
where $\gamma$ is a number that usually takes values between $0$ and $2$ and $\delta$ is a number that usually takes values between $0$ and $5$. 

The figure below illustrates the form of this transformation when $\gamma = .5$, $\gamma = .1$ or $\gamma = 1.5$ (line colors), and when $\delta = .4$, $\delta = 1.2$ or $\delta=2$ (different plot facets).

```{r weighting-function-demo}
#| fig-width: 10
#| fig-height: 3

wf <- expand_grid(p = c(seq(0, 1, .01)) , 
                  gamma = c(.5, 1, 1.5) , 
                  delta = c(.4, 1, 2)) %>%
  mutate(w = ( delta*(p^gamma) ) / (  (delta*(p^gamma)) + (1-p)^gamma ) )


wf %>% ggplot(aes(p, w, group = as.factor(gamma), color = as.factor(gamma))) +
  facet_wrap(~delta, nrow=1) + 
  geom_abline(slope=1, linetype='dashed', linewidth = 1, color = 'gray') +
  geom_line(linewidth = 1) + 
  theme_minimal()
```

It can be seen that in the case of $\gamma = \delta =  1$ (green line, middle panel), all decision weights $w(p)$ are equal to the objective probabilities $p$. 
However, when $\gamma \neq 1$ and/or $\delta \neq 1$, the function gets curved, indicating a nonlinear transformation of probabilities.  

The exact form of the transformation depends on the combination of $\gamma$ and $\delta$. 
Generally, the function takes an inverse S-shape when  $\gamma < 1$ (red line), but an S-shape when $\gamma > 1$ (red line). 
$\gamma$ is therefore often referred to as the curvature parameter and reflects probability sensitivity - how accurately objective probabilities are represented.
When $\delta = 1$, the function intersects the diagonal at the probability midpoint $p=.5$. The function is more elevated (runs above the diagonal most of the time) when $\delta > 1$, but less elevated (runs below the diagonal most of the time) when $\delta < 1$. Consequently, $\delta$ is the *elevation parameter* and is sometimes psychologically interpreted as optimism or pessimism.  

In CPT, the transformations of the probability weighting function are used to derive the decisions weights $\pi$. However, in some applications, the function might differ depending on whether a lottery contains losses or gains (more on that later).

When each option has at most two different outcomes that are all in the gain domain, the decisions weights are derived according to:  

$$
\begin{align}
\pi_{high}^+ &= w(p_{high}) \\
\pi_{low}^+ &= 1 - \pi_{high}^+ 
\end{align}
$$
When each option has at most two different outcomes that are all in the loss domain, the decisions weights are derived according to:   

$$
\begin{align}
\pi_{low}^- &= w(p_{low}) \\
\pi_{high}^- &= 1 - \pi_{low}^+ 
\end{align}
$$
The subjective values and decision weights that are obtained via these two transformations are then used to compute the subjective valuations of each option. 
Similar to EV maximization, it is assumed that the option with the highest valuation is chosen. 

Here, we first rank-order the outcomes and their probabilities according to the magnitude of the outcomes.

```{r}
cpt <- problems %>% 
  mutate( # rank order attributes
    hA = case_when(outcomeA1 > outcomeA2 ~ outcomeA1, 
                   outcomeA1 < outcomeA2 ~ outcomeA2, 
                   outcomeA1 == outcomeA2 ~ outcomeA1) , 
    lA = case_when(outcomeA1 > outcomeA2 ~ outcomeA2, 
                   outcomeA1 < outcomeA2 ~ outcomeA1,
                   outcomeA1 == outcomeA2 ~ outcomeA2) ,
    hB = case_when(outcomeB1 > outcomeB2 ~ outcomeB1, 
                   outcomeB1 < outcomeB2 ~ outcomeB2, 
                   outcomeB1 == outcomeB2 ~ outcomeB1) , 
    lB = case_when(outcomeB1 > outcomeB2 ~ outcomeB2, 
                   outcomeB1 < outcomeB2 ~ outcomeB1,
                   outcomeB1 == outcomeB2 ~ outcomeB2) ,
    p_hA = case_when(outcomeA1 > outcomeA2 ~ probA1,
                     outcomeA1 < outcomeA2 ~ probA2, 
                     outcomeA1 == outcomeA2 ~ probA1) , 
    p_lA = case_when(outcomeA1 > outcomeA2 ~ probA2,
                     outcomeA1 < outcomeA2 ~ probA1,
                     outcomeA1 == outcomeA2 ~ probA2) ,
    p_hB = case_when(outcomeB1 > outcomeB2 ~ probB1,
                     outcomeB1 < outcomeB2 ~ probB2,
                     outcomeB1 == outcomeB2 ~ probB1) , 
    p_lB = case_when(outcomeB1 > outcomeB2 ~ probB2, 
                     outcomeB1 < outcomeB2 ~ probB1,
                     outcomeB1 == outcomeB2 ~ probB2)) %>%
  select(!(probA1:outcomeB2)) %>% 
  select(problem, label, domain, hA:p_lB, ev_A, ev_B, max_ev_det, ev_max_obs, everything()) 
```



```{r}
#| echo: false

cpt_problems <- cpt %>% select(problem:p_lB)
cpt_choices <- cpt %>% select(`1_er171989`:`99_AA211989`)
```

We then determine what choice CPT predicts for each choice problem given a set of parameter values that we can set. Here, we first illustrate a case where all CPT parameters equal 1. 

```{r}

lambda <- 1
alpha <- 1
gamma <- 1
delta <- 1

cpt_pred <- cpt %>%
  
  mutate(
    
    ### transform attributes
    
    # value function: transformation of outcome values
    v_hA = if_else(hA >= 0, hA^alpha ,  -lambda*(abs(hA)^alpha)) , 
    v_lA = if_else(lA >= 0, lA^alpha ,  -lambda*(abs(lA)^alpha)) , 
    v_hB = if_else(hB >= 0, hB^alpha ,  -lambda*(abs(hB)^alpha)) ,
    v_lB = if_else(lB >= 0, lB^alpha ,  -lambda*(abs(lB)^alpha)) , 
    
    # probability weighting function: transformation of probabilities
    pi_hA = case_when(hA >= 0 ~ ( delta*(p_hA^gamma) ) / (  (delta*(p_hA^gamma)) + p_lA^gamma ) , 
                      hA < 0 ~ 1 - ( delta*(p_lA^gamma) ) / (  (delta*(p_lA^gamma)) + p_hA^gamma ) ) ,
    pi_lA = 1 - pi_hA , 
    
    pi_hB = case_when(hB >= 0 ~ ( delta*(p_hB^gamma) ) / (  (delta*(p_hB^gamma)) + p_lB^gamma ) , 
                      hB < 0 ~ 1 - ( delta*(p_lB^gamma) ) / (  (delta*(p_lB^gamma)) + p_hB^gamma )
    ) ,  
    pi_lB = 1 - pi_hB ,
    
    ### option valuation and choice
    
    # Calculates the subjective valuation of each option in a choice problem.
    cpt_A = pi_hA*v_hA + pi_lA*v_lA ,   
    cpt_B = pi_hB*v_hB + pi_lB*v_lB ,
    
    # Compare option valuations and deterministically predict a choice based on this comparison.
    cpt_det = case_when(cpt_A > cpt_B ~ 1 , 
                        cpt_A < cpt_B ~ 0 , 
                        cpt_A == cpt_B ~ NA )
    ) 
    
# here, we calculate the proportion of people whose choice aligns with CPT's predictions.
test <- cpt_pred %>% 
  mutate(cpt_max_obs = round ( case_when(cpt_det == 1 ~ rowMeans(select(., `1_er171989`:`99_AA211989`), na.rm = TRUE)  , 
                                         cpt_det == 0 ~ 1-rowMeans(select(., `1_er171989`:`99_AA211989`), na.rm = TRUE) 
                                        ) , 2
                              )
         ) %>%
  select(problem, label, domain, ev_max_obs, cpt_max_obs) %>% 
  mutate(equal = ev_max_obs==cpt_max_obs)

# here, we output the mean of empirical choices aligning with EV maximization and with CPT predictions, respectively.
mean(test$ev_max_obs, na.rm=T) 
mean(test$cpt_max_obs, na.rm=T) 
```

As you see, the proportion of choices aligning with CPT does not differ from the proportion aligning with EV maximization. When all parameters equal 1, outcome values and probabilities are not distorted and CPT makes the same predictions as EV maximization.

If, however, we choose different parameters that will yield curved value and weighting functions; the CPT predictions differ from EV maximization.
Here, we show this based on an empirically plausible set of parameter values (given previous research) and calculate the proportion of people for each choice problem in the empirical data from Kellen et al. whose choice aligned with CPT's predictions:

```{r}

lambda <- 1.2
alpha <- 0.8
gamma <- 0.6
delta <- 1

cpt_pred <- cpt %>%
  
  mutate(
    
    ### transform attributes
    
    # value function: transformation of outcome values
    v_hA = if_else(hA >= 0, hA^alpha ,  -lambda*(abs(hA)^alpha)) , 
    v_lA = if_else(lA >= 0, lA^alpha ,  -lambda*(abs(lA)^alpha)) , 
    v_hB = if_else(hB >= 0, hB^alpha ,  -lambda*(abs(hB)^alpha)) ,
    v_lB = if_else(lB >= 0, lB^alpha ,  -lambda*(abs(lB)^alpha)) , 
    
    # probability weighting function: transformation of probabilities
    pi_hA = case_when(hA >= 0 ~ ( delta*(p_hA^gamma) ) / (  (delta*(p_hA^gamma)) + p_lA^gamma ) , 
                      hA < 0 ~ 1 - ( delta*(p_lA^gamma) ) / (  (delta*(p_lA^gamma)) + p_hA^gamma ) ) ,
    pi_lA = 1 - pi_hA , 
    
    pi_hB = case_when(hB >= 0 ~ ( delta*(p_hB^gamma) ) / (  (delta*(p_hB^gamma)) + p_lB^gamma ) , 
                      hB < 0 ~ 1 - ( delta*(p_lB^gamma) ) / (  (delta*(p_lB^gamma)) + p_hB^gamma )
    ) ,  
    pi_lB = 1 - pi_hB ,
    
    ### option valuation and choice
    
    # Calculates the subjective valuation of each option in a choice problem.
    cpt_A = pi_hA*v_hA + pi_lA*v_lA ,   
    cpt_B = pi_hB*v_hB + pi_lB*v_lB ,
    
    # Compare option valuations and deterministically predict a choice based on this comparison.
    cpt_det = case_when(cpt_A > cpt_B ~ 1 , 
                        cpt_A < cpt_B ~ 0 , 
                        cpt_A == cpt_B ~ NA )
    ) 
    
# here, we calculate the proportion of people whose choice aligns with CPT's predictions.
test <- cpt_pred %>% 
  mutate(cpt_max_obs = round ( case_when(cpt_det == 1 ~ rowMeans(select(., `1_er171989`:`99_AA211989`), na.rm = TRUE)  , 
                                         cpt_det == 0 ~ 1-rowMeans(select(., `1_er171989`:`99_AA211989`), na.rm = TRUE) 
                                        ) , 2
                              )
         ) %>%
  select(problem, label, domain, ev_max_obs, cpt_max_obs) %>% 
  mutate(equal = ev_max_obs==cpt_max_obs)


# here, we output the mean of empirical choices aligning with EV maximization and with CPT predictions, respectively.
mean(test$ev_max_obs, na.rm=T) 
mean(test$cpt_max_obs, na.rm=T) 
```

Comparing the mean of choices that align with EV maximization vs. CPT, we see that CPT better predicts our empirical choices given the chosen set of parameter values. This is a nice improvement, although small. The obvious question is: could this prediction accuracy be improved even further if we chose other parameter values? How can we find out which parameter values fit best?

## Fitting CPT

In the above demonstration of CPT, we showed that applying the transformations implemented in CPT lead to an improved prediction over EV maximization. 
The behavior of the curves for different parameter values constrains the plausible (combinations of) parameter values to some degree - but in the previous examples, we nonetheless set their values in a rather unsystematic way. 
Hence, it is likely that another combination of parameters might yield a better prediction.  


But how to find the best fitting combination of parameter values $\Theta$ for our model? 
In other words, how do we get from our initial beliefs about the plausibility of the parameter values, $p(\Theta) = p(\alpha, \lambda, \gamma, \delta)$, (before having seen the data), to updated "beliefs" about the parameters values after having seen the data $p(\Theta|D)$? 

$$
p(\Theta) \to p(\Theta|D) \; \text{?}
$$
Here is the intuition behind the solution for this updating problem from a *Bayesian* perspective:

1. Take a set of parameter values for the parameters in your model
2. Calculate the probability of the data given these parameter values (likelihood) 
3. Weigh (multiply) the likelihood with the initial plausibility of parameter values (the prior)

The product we obtain from Step 3 (i.e., the multiplication of prior and likelihood) gives us the updated probability of the respective parameter values given the data. This is called the *posterior probability*: 

$$
p(\Theta|D) \propto p(\Theta) p(D|\Theta)
$$ 
If we repeat these steps for all possible combinations of parameter values, we can simply take the parameter values that have the biggest posterior probability given the data.
Before demonstrating how we can compute these posteriors for our CPT model using `R` and a simulation program calls `JAGS`, we explain the different parts of this Bayesian updating process in more detail.

### Prior

The prior probability, $p(\Theta)$ expresses our initial beliefs about the plausibility of different parameter values. For example, before testing if a coin (or dice) is biased, you would probably have a prior belief that the coin has a .50 probability of landing on each side (or a 1/6 = .17 probability for each side of the classical dice). Additionally, you will probably find it unlikely that any side has a probability of >.90 of landing on it, even if the coin or dice is biased (that would defy the law of physics if coin landed on heads if you let it fall to the ground with heads up from a height of 10 cm). 
The main function of the prior is to indicate which parameter values could be obtained in principle (the possible range of values), and how plausible the values on this range are relatively to each other.
In other words, the prior is a probability distribution over parameter values. 
<!--maybe we can get more detailed here, e.g., provide an example-->

### Likelihood

#### The Most Simple Choice Model

The likelihood is the probability of the data given the model and a set of parameter values.

To illustrate, a much simpler model than CPT for the risky choice problems from Kellen et al. (2016) would be a binomial model which predicts the choice of `Option 1` over `Option 2` with a constant probability $\theta$.
That is, when $\theta=1$, it always predicts choosing `Option 1`. 
As a consequence, when someone chooses `Option 2`, the probability of this choice given our model would be $0$. 
In contrast, if instead $\theta=.3$, the probability of someone choosing `Option 2` would be instead $.7$. 
In other words, the observed choice of `Option B` is more likely when $\theta = .3$ than when $\theta=1$. 
Such a model of course disregards any attributes (outcome values and probabilities) of a given choice problem.

```{r}
# theta=1
theta <- 1
B_choice <- 0 
dbinom(B_choice,1,prob=theta)

# theta = .3
theta <- .3
B_choice <- 0
dbinom(B_choice,1,prob=theta)
```

We can now extend this example to more choices and more candidate values for $\theta$. 
Note that the likelihood for multiple choices is simply the product of all choices' individual likelihoods, assuming that the choices are independent.
In the following example, we take the candidate values `\theta=``r seq(0,1,.1)` and calculate the likelihood of the choices of all participants on Problem 1 from Kellen et al. (2016) data given these parameter values. 

```{r}
# get choices from problem 1
choices_p1 <- as.numeric(as.vector(cpt_choices[1,]))
choices_p1

# calculate the likelihood
theta <- seq(0,1,.1)
lh <- round(dbinom(sum(choices_p1),length(choices_p1), prob=theta), 3)
tibble(Theta=theta , 
       Likelihood = lh, 
       Likelihood_std = lh/sum(lh)) %>%
  kable()
```

We can easily see that the observed choices on Problem 1 are much more likely given the data when we assume a moderate to high probability of choosing `Option 1` (parameter values between 0.7 and 0.8) when we rely on this simple binomial decision model.

#### Likelihood for CPT

Although CPT is a more complex model than the simple binomial from above, the idea is exactly the same. 
We have a number of free (unknown) parameters that we want to learn about ($\theta$ for the binomial; $\alpha, \lambda,\gamma,\delta$ for CPT as outlined above) and the probability of observed choices depends on the values of these parameters.
The main difference between the binomial and CPT is that the binomial simply assumes a constant probability for choosing `Option 1` vs. `Option 2` for all choice problems (that is $\theta$), independent of characteristics of the choice problems, whereas CPT uses (transformations of) the outcomes and their probabilities in a choice problem to compute a choice probability that is specific for this problem. 

So far, CPT can only make deterministic predictions (e.g., always choose `Option A` when $V_A > V_B$). Assuming a probabilistic world, where people mostly choose according to their overall preference, but may sometimes also act differently depending on context (e.g., peer pressure or other factors), we need to make one more adjustment to CPT to make it probabilistic instead of deterministic. Doing so, CPT will be able to provide us with choice probabilities that can also be different from 0 and 1.


In this adjustment, we use a choice rule, which predicts a higher probability for choosing an option the higher the *difference between the two option's valuations* is (e.g., higher probability for choosing `Option A` the better its CPT score $V_A$ is compared to the score for `Option B`). 
That is, in contrast to the deterministic version, the prediction does not only depend on *if* `Option A` obtains a better evaluation than `Option B` (or a worse one), but also on *how much* better (or *how much* worse) this evaluation is. 
If the evaluation is much better, it is highly likely that `Option A` is chosen, if it is much worse, it is instead highly likely that `Option B` is chosen. 
If their evaluations are similar however, the choice probability gets closer to $.5$, indicating that a person might be indifferent between the two choice options.
This logic is implemented in the so-called *logit choice rule*, which takes the CPT valuations as input and returns a choice probability as output: 
$$
p(A) = \frac{1}{1+e^{-\phi(V_A-V_B)}} \: , 
$$

where $\phi$ is a number that can take values $\geq0$ (free parameter).
The figure below illustrates how the logit choice rule translates differences in the CPT valuations (x-axis) into a choice probability that is constrained between 0 and 1 (y-axis).
In general, when the difference in valuations is 0, the choice probability is .5. 
This reflects the case that when the options do not really differ, we also have no intuitive preference for one option or the other. 
Then, when the difference in valuations becomes positive, the probability for choosing `Option A` *increases*; and when the difference becomes negative, the probability for choosing `Option A` *decreases*.
As can be seen, $\phi$ affects how strongly the choice probability is influenced by the difference magnitude between the option's valuations. 

```{r logit-demo}
logit <- expand_grid(Diff = c(seq(-4, 4, .01)) , 
                     phi = c(0, .5, 1)) %>%
  mutate(Prob = 1/(1+(exp(-phi*Diff)) ))


logit %>% ggplot(aes(Diff, Prob, group = as.factor(phi), color = as.factor(phi))) +
  geom_line(linewidth = 1) + 
  theme_minimal()
```

### Using MCMC/JAGS to obtain the posterior


** LINUS EXPANDS ON HOW MCMC WORKS WITH FEW EXAMPLES HERE **


## Fitting the Model to description-based choices from Kellen et al. (2016)

Now how does MCMC sampling for model fitting work in practice? We can conventiencly use JAGS through R. To use it, first download and install JAGS on your computer via [this link](https://sourceforge.net/projects/mcmc-jags/). Mac users might need to additionally install the GNU Fortran (gfortran-4.2.3.dmg) library from the [CRAN tools
directory](https://cran.r-project.org/bin/macosx/tools/) - please check if it works without it.

Now start the Terminal (Mac) or Console (Windows) (not RStudio!) and type:

`jags`

If JAGS is installed successfully, you will receive a message like the following:   
`Welcome to JAGS 4.3.1 (official binary) on Fri May  2 17:27:44 2025`  
`JAGS is free software and comes with ABSOLUTELY NO WARRANTY`

Now, quit the Terminal again.   

Then, open RStudio and install R2jags (if not installed yet, outcommented below) and load it.


```{r}
#| output: false

# install.packages("R2jags", dependencies = TRUE)
library(R2jags)

```


Now we can go on to prepare the data for the model fitting. Generally, a JAGS model is written in a way that it expects us to hand over the data in a certain format, to be able to fit the model to the data. A JAGS model is specified in a text file and includes all the mathematical CPT formulae we discussed before in a code format.  
In our case, the first model that we want to fit is called `JAGS_cpt_model_individual.txt`. Let's first have a look at the model code:

```
# JAGS_cpt_model_individual.txt

model {

  for (j in 1:nSubj) {  #Subject-loop

    # Priors
    alpha.pre[j] ~ dbeta(1,1)
    lambda.pre[j] ~ dbeta(1,1)
    gamma.pre[j] ~ dbeta(1,1)
    delta.pre[j] ~ dbeta(1,1)
    rho.pre[j] ~ dbeta(1,1) 
    
    alpha[j] <- (alpha.pre[j]) * 2
    lambda[j] <- (lambda.pre[j]) * 5
    gamma[j] <- (gamma.pre[j]) * 2
    delta[j] <- (delta.pre[j]) * 5
    rho[j] <- (rho.pre[j]) * 5 
  

    #Gains
    for (i in 1:nGain){  #Item loop

    
      #Value function
      #Lottery A
      v.hA[i,j] <- pow(hA[i],alpha[j])
      v.lA[i,j] <- pow(lA[i],alpha[j])
      
      #Lottery B
      v.hB[i,j] <- pow(hA[i],alpha[j])       
      v.lB[i,j] <- pow(hB[i],alpha[j])
      
      #Probability weighting function 
      #Lottery A
      pi.hA[i,j] <- (delta[j] * (pow(p_hA[i], gamma[j]))) / (delta[j] * (pow(p_hA[i], gamma[j])) + pow(p_lA[i], gamma[j]))
      pi.lA[i,j]<- 1 - pi.hA[i,j]
      
      #Lottery B
      pi.hB[i,j] <- (delta[j] * (pow(p_hB[i], gamma[j]))) / (delta[j] * (pow(p_hB[i], gamma[j])) + pow(p_lB[i], gamma[j]))
      pi.lB[i,j] <- 1 - pi.hB[i,j]
      
      #Valuation 
      Vf.A[i,j]  <- pi.hA[i,j] * v.hA[i,j] + pi.lA[i,j] * v.lA[i,j]
      Vf.B[i,j]  <- pi.hB[i,j] * v.hB[i,j] + pi.lB[i,j] * v.lB[i,j]
      
      #Choice rule
      binval[i,j] <- (1)/(1+exp(-1*(rho[j]*(Vf.A[i,j]-Vf.B[i,j]))))
      choice[i,j] ~ dbern(binval[i,j])
      
    }
    
    
    
    # Losses
    for (i in (nGain+1):(nGain+nLoss)) {
    
    
      #Value function
      #Lottery A
      v.hA[i,j] <- lambda[j]*(-1) * pow((-1*hA[i]),alpha[j])
      v.lA[i,j] <- lambda[j]*(-1) * pow((-1*lA[i]),alpha[j])
  
      #Lottery B
      v.hB[i,j] <- lambda[j]*(-1) * pow((-1*hB[i]),alpha[j])     
      v.lB[i,j] <- lambda[j]*(-1) * pow((-1*hB[i]),alpha[j])
      
      #Probability weighting function 
      #Lottery A
      pi.lA[i,j] <- (delta[j] * (pow(p_lA[i], gamma[j]))) / (delta[j] * (pow(p_lA[i], gamma[j])) + pow(p_hA[i], gamma[j]))
      pi.hA[i,j] <- 1 - pi.lA[i,j]
  
      #Lottery B
      pi.lB[i,j] <- (delta[j] * (pow(p_lB[i], gamma[j]))) / (delta[j] * (pow(p_lB[i], gamma[j])) + pow(p_hB[i], gamma[j]))
      pi.hB[i,j] <- 1 - pi.lB[i,j]
  
      #Valuation 
      Vf.A[i,j]  <- pi.hA[i,j] * v.hA[i,j] + pi.lA[i,j] * v.lA[i,j]
      Vf.B[i,j]  <- pi.hB[i,j] * v.hB[i,j] + pi.lB[i,j] * v.lB[i,j]
  
      #Choice rule
      binval[i,j] <- (1)/(1+exp(-1*(rho[j]*(Vf.A[i,j]-Vf.B[i,j]))))
      choice[i,j] ~ dbern(binval[i,j])
      
      }
      
      
      # Mixed
       
      for (i in (nGain+nLoss+1):nTotal) {
    
    
      #Value function
      #Lottery A
      v.hA[i,j] <- pow(hA[i],alpha[j])
      v.lA[i,j] <- lambda[j]*(-1) * pow((-1*lA[i]),alpha[j])
  
      #Lottery B
      v.hB[i,j] <- pow(hB[i],alpha[j])   
      v.lB[i,j] <- lambda[j]*(-1) * pow((-1*lB[i]),alpha[j])
  
      #Probability weighting function 
      #Lottery A
      pi.hA[i,j] <- (delta[j] * (pow(p_hA[i], gamma[j]))) / (delta[j] * (pow(p_hA[i], gamma[j])) + pow(p_lA[i], gamma[j]))
      pi.lA[i,j]<- 1 - pi.hA[i,j]
      
      #Lottery B
      pi.hB[i,j] <- (delta[j] * (pow(p_hB[i], gamma[j]))) / (delta[j] * (pow(p_hB[i], gamma[j])) + pow(p_lB[i], gamma[j]))
      pi.lB[i,j] <- 1 - pi.hB[i,j]
  
      #Valuation 
      Vf.A[i,j]  <- pi.hA[i,j] * v.hA[i,j] + pi.lA[i,j] * v.lA[i,j]
      Vf.B[i,j]  <- pi.hB[i,j] * v.hB[i,j] + pi.lB[i,j] * v.lB[i,j]
  
      #Choice rule
      binval[i,j] <- (1)/(1+exp(-1*(rho[j]*(Vf.A[i,j]-Vf.B[i,j]))))
      choice[i,j] ~ dbern(binval[i,j])
      
      }
  }
}    

```

As you see in the model code, there are different sections. In the major part of the model (starting from where it says "Item Loop"), the main equations of CPT are included, as we have introduced and discussed them earlier. Notably, there are three different sections in which the formulae are repeated (more or less): One section for pure Gain problems, one for pure Loss problems, and one for Mixed problem. The reason for this is that you may have different parameters for the different domains: for example, $\lambda$ is only reflected in the value function formula in the loss domain:  

```
v.hA[i,j] <- lambda[j]*(-1) * pow((-1*hA[i]),alpha[j])
v.lA[i,j] <- lambda[j]*(-1) * pow((-1*lA[i]),alpha[j])
```

but not in the Gain domain:

```
v.hA[i,j] <- pow(hA[i],alpha[j])
v.lA[i,j] <- pow(lA[i],alpha[j])
```

That's why we have to loop over the items (problems) from one domain after the other:

```
# Losses
for (i in (nGain+1):(nGain+nLoss)) {...}
```

This executes the respective domain-specific model code for every problem in a given domain.
To be able to do so, we have to count the number of problems for each domain in the data, such that we can tell the model how long it should iterate each loop:

```{r }
nGain <- cpt_problems %>% filter(domain=='Gain') %>% nrow()
nLoss <- cpt_problems %>%  filter(domain=='Loss') %>% nrow()
nMixed <- cpt_problems %>%  filter(domain=='Mixed') %>% nrow()
nTotal <- cpt_problems %>% nrow()
nSubj <- ncol(cpt_choices)
```

The domain-specific model code for each problem ends with the following part: 

```
#Valuation 
Vf.A[i,j]  <- pi.hA[i,j] * v.hA[i,j] + pi.lA[i,j] * v.lA[i,j]
Vf.B[i,j]  <- pi.hB[i,j] * v.hB[i,j] + pi.lB[i,j] * v.lB[i,j]
  
#Choice rule`
binval[i,j] <- (1)/(1+exp(-1*(rho[j]*(Vf.A[i,j]-Vf.B[i,j]))))
choice[i,j] ~ dbern(binval[i,j])
```

Which determines the subjective valuation of each option in a choice problem, and then computes the choice probability of choosing `Option A` in the choice rule. 
It then compares it with our empirical choice data that - according to this model - JAGS expects to be handed over in a two-dimensional object of `choice[i,j]`. For the CPT formulae, you saw that `hA[i]`, `lA[i]`, `p_hA[i]` etc. are one-dimensional objects that we have to hand over from our R environment.

For the choice sensitivity parameter that is part of the choice rule, note that there is substantial inconsistency in the literature how it is called: here it is called rho, sometimes it is also called theta or phi.


You might have wondered what the code means that the model starts with: 

```
    # Priors
    alpha.pre[j] ~ dbeta(1,1)
    lambda.pre[j] ~ dbeta(1,1)
    gamma.pre[j] ~ dbeta(1,1)
    delta.pre[j] ~ dbeta(1,1)
    rho.pre[j] ~ dbeta(1,1) 
    
    alpha[j] <- (alpha.pre[j]) * 2
    lambda[j] <- (lambda.pre[j]) * 5
    gamma[j] <- (gamma.pre[j]) * 2
    delta[j] <- (delta.pre[j]) * 5
    rho[j] <- (rho.pre[j]) * 5 
```

These are our priors. In this model, we use uninformative priors (assigning a uniform beta distribution with equal a priori probability across all possible values for all parameters), but we constrain our parameters to empirically plausible ranges. To do so,  we multiply the beta distribution (initially ranging from 0 to 1) by a constant to constrain the parameter between 0 and 2 or between 0 and 5, respectively.
  
Now that you have an overview of how the model is structured, let's prepare the rest of our data for JAGS.
As you saw above, the JAGS model expects two-dimensional (choices) and one-dimensional (problem attributes) data formats for our variables. We now create a list called `data`that includes several entries with all our data in this exact expected format.

```{r }
data <- list(choice = as.matrix(cpt_choices) ,
             hA = cpt_problems$hA ,
             lA = cpt_problems$lA ,
             hB = cpt_problems$hB ,
             lB = cpt_problems$lB ,
             p_hA = cpt_problems$p_hA ,
             p_lA = cpt_problems$p_lA ,
             p_hB = cpt_problems$p_hB ,
             p_lB = cpt_problems$p_lB ,
             nGain = nGain , 
             nLoss = nLoss , 
             nTotal = nTotal , 
             nSubj = nSubj
  )
```

Lastly, we can specify some initial values to facilitate sampling - these are the values that JAGS starts with in each chain. In this model, we estimate individual parameters for every participant, so we have to provide as many starting values as we have participants.

```{r }
#source("helper_functions/fun_initialize_MCMC.R") # calls function to create starting values for MCMC
params_init <- function(){
  list("alpha.pre" = rep(.5, nSubj) , # individual level parameters
       "lambda.pre" = rep(.5, nSubj) ,
       "gamma.pre" = rep(.5, nSubj) ,
       "delta.pre" = rep(.5, nSubj) , 
       "rho.pre" = rep(.001, nSubj) 
       ) 
}
```

Almost good to go. All that's left now is to tell JAGS which parameters we're interested in (i.e., which parameters JAGS should record and provide us a summary with. The more parameters you mention here, the bigger the resulting object will be):

```{r }
params_cpt <- c("alpha", "lambda", "gamma", "delta", "rho") # free parameters
```


Now we'll start fitting the model. The function `jags.parallel` enables the use of several Computer cores at once, making it possible to save time by sampling several chains in paralell. The number of cores used is specified in the argument `n.cluster`, the number of chains is specified in `n.chains`. Here, each chain has 2000 iterations, but JAGS only starts recording samples after a `n.burnin` period of 1000 samples. This is because it takes a while until the chain has "found" a range of values that actually make sense, roughly speaking. `n.thin` is the so-called thinning parameter that controls how many samples are recorded from the chain. In this case, we set it to 2, meaning that only every second sample is recorded after a `n.burnin` phase of 1000 iterations. This is done to reduce so-called *autocorrelation* of the samples from the chains.

Since it can take a while to fit the model, here you can also load the RDS file with the samples from when we fitted the model.

```{r }

# if RDS file exists, load existing fits 
if(file.exists(here("BayesianCognitiveModeling", "modelfits", "cpt_model.rds"))){
  samples <- readRDS(here("BayesianCognitiveModeling", "modelfits", "cpt_model.rds")) 
  print(paste("model loaded"))
  
# if RDS file doesn't exist, rerun fits:
} else {

## sample from posterior distributions using MCMC
samples <- jags.parallel(data = data , # empirical data
                   inits = params_init , # initial values for parameters in each chain
                   parameters.to.save = params_cpt , # parameters we want to record
                   model.file = here("BayesianCognitiveModeling", "JAGS_models", "JAGS_cpt_model_individual.txt"), # model file
                   n.chains = 6, # number of chains 
                   n.iter = 2000 , # number of iterations per chain
                   n.burnin = 1000 , # burn-in period
                   n.cluster = 6 , # number of cores to use: compute MCMC chains in parallel
                   n.thin = 2,
                   DIC = TRUE , 
                   jags.seed = 1223) # seed to enable reproducibility

saveRDS(samples, here("BayesianCognitiveModeling", "modelfits", "cpt_model.rds")) # once the model is done fitting, we first save the samples to make sure they don't get lost.
print(paste("done fitting model to data"))
}


```


Let's inspect the `samples` object. All single samples are recorded in the `sims.list`of the `BUGSoutput` element of the `samples` object. For example, all ((2000 iterations - 1000 burnin samples) * 6 chains = 6000) samples for each of the (104) individual $\alpha$ parameters are saved here:

```{r}

str(samples[["BUGSoutput"]][["sims.list"]][["alpha"]])

```

The mean or median for each parameter (a point estimate for each participant's parameter) is also saved explicitly:

```{r}
samples[["BUGSoutput"]][["median"]][["alpha"]]

samples[["BUGSoutput"]][["mean"]][["alpha"]]

```

We see that the parameter estimates for every individual vary quite substantially.

We can also have a look at some more summary characteristics:

```{r}
kable(head(samples$BUGSoutput$summary)) 

```

Here, we see for every individual parameter the `mean` (as we just looked at them right before), but also the credibility intervals of the posterior distribution. If you want to look at the posterior distributions of some parameters specifically, you can plot them using commands from the bayesplot package:

```{r posterior-plot}
# install.packages("bayesplot")
bayesplot::mcmc_dens(samples$BUGSoutput$sims.array, pars = "alpha[1]") + theme_classic()
```


Importantly, we also get information on the effective sample size `n.eff` and `Rhat`. `n.eff` is an estimate of how many “independent” draws your correlated chains are worth. `Rhat` is basically a measure for convergence (how well your different chains mix and converge on the same value distribution). Typically, values below 1.01 indicate good convergence. Let's check if that's the case here by ranking the rows from highest to lowest Rhat:

```{r}
as.data.frame(samples$BUGSoutput$summary) %>% 
  arrange(desc(Rhat)) %>% 
  head() %>% kable()

```

We see that for some parameters, the chains have not mixed well (Rhats up to 1.37). Visually, this can also be checked by looking at traceplots (plotting all chains' sampled values across the iterations). Good convergence (low Rhats) will be resembled visually by traceplots which look like "hairy caterpillars": For example, this is the case for the 98th individual's delta parameter, whereas the 59th individual's alpha parameter shows particularly bad convergence:


```{r mcmc-plot}
bayesplot::mcmc_trace(
  samples$BUGSoutput$sims.array,
  pars = c("delta[98]", "alpha[59]"),
  facet_args = list(ncol = 1)    
) + ggplot2::scale_color_discrete()

```

One possible way to improve convergence would be to use longer chains (more iterations per chain).

# ** POTENTIALLY EXPAND ON BUGSOUTPUT HERE **

Now that we have our individual parameter estimates, let's plot the value and weighting function and we see the typical patterns of value und probability distortion that were predicted by CPT. To do so, we call a helper function that we have written for you in a seperate script. You have to provide the samples object returned by JAGS as an argument in case it has a different name. First, we plot the value function:


```{r value-function-ind}
source(here("BayesianCognitiveModeling","helper_functions", "CPT_plotting.R")) # calls functions for plotting
v_fun_TK92(samples = samples)

```

... and we see that even though there is some interindividual variability, all participants show the distortion of outcome values as predicted by CPT.

Next, we plot the probability weighting function:


```{r weighting-function-ind}
w_fun_GE87(samples = samples)
```

... and again we see that even though there is some interindividual variability, all participants show the distortion of objective probabilities as predicted by CPT.


# Fitting a hierarchical Model to experience-based choices from Kellen et al. (2016)

To illustrate the flexibility of computational modeling and its different facets, we want to conclude with a second example. In comparison to our previous modeling approach, we make three key changes:

1. Modeling experience-based choice data
2. Fitting a different probability weighting function
3. Fitting a *hierarchical* implementation of CPT

### Experience-based choices

First, we want to model different empirical data. Additionally to the description-based choices modeled above, Kellen et al. also had a condition in which participants performed the so-called sampling paradigm: Here, participants are not presented with explicit summary statistics about the option's payoff distributions, but they have to infer them via experiental sampling. This is why choices from this paradigm are also called *decisions from experience* (DFE). Specifically, in the sampling paradigm, participants first freely draw samples from the options until they feel ready to make a consequential choice between the options. For more details on these different paradigms, see [Hertwig & Erev (2009)](https://linkinghub.elsevier.com/retrieve/pii/S1364661309002125).

Let's have a look at the sampling data, and we'll see that it varies across problems and participants if and how samples are drawn from each option (`SampVal`) before a choice is made (`response`):

```{r}
samples_kellen<- openxlsx::read.xlsx(here("BayesianCognitiveModeling", "kellen2016.xlsx"), sheet = "Sampling_experience") %>% # read in sampling info
  dplyr::rename(ProblemName = ProblemID, ProblemID = ProblemName)
head(samples_kellen, n = 20 ) %>% kable()
```

Based on this sampling data, "subjective probabilities" are calculated in DFE from the relative frequency by which outcomes are encountered for each option. For example, we saw above that subject 1 (ParticipantCode `1_er171989 `) has only sampled once from each option in problem 47 before making a choice. Thus, the below subjective probabilities result for this problem and this particular participant, where `option A` seemingly has a sure outcome of 130 and `option B` seemingly has a sure outcome of 190: 

```{r}
#| output: false

problems_exp <- read_xlsx(here("BayesianCognitiveModeling", "kellen2016.xlsx"), sheet = "Lottery_problems_experienced") %>% # read in choice problems
  dplyr::rename(probA3 = 8, outcomeA3 = 9, probB3 = 14, outcomeB3 = 15) # add missing variable names for third outcomes
# problems_exp[is.na(problems_exp)==T] <- 0
```

```{r}
problems_exp %>% 
  filter(ParticipantCode == "1_er171989" & ProblemName == 47) %>% 
  kable()

```

Note, however, that of course the actual underlying *ground-truth* payoff distributions of the choice problems are identical to that of the problems in the description-based choices we started with in the beginning. But since the participants may have a different impression of the outcome's frequencies based on their sampling behavior, we use the calculated subjective probabilities for modeling.

It follows that in DFE, the problem characteristics (particularly the subjective probabilities) may differ between participants for each choice problem.
This is why we also have to structure the data a bit more carefully before handing them over to JAGS. We'll get to that in a moment.     

Next, let's have a look at our new model `JAGS_cpt_model_hierarchical_TK92-onedomain.txt`, to know what data structure it expects.

```
# JAGS_cpt_model_hierarchical_TK92-onedomain.txt


model {
  for (j in 1:nSubj) {  #Subject-loop

  #### PRIORS FOR INDIVIDUAL LEVEL
    #Value function 
    alpha.phi[j] ~ dnorm(mu.phi.alpha, tau.phi.alpha)T(-5,5)
    alpha.pre[j] <- phi(alpha.phi[j])
    alpha[j] <- alpha.pre[j]*2

    #Probability weighting function
    gamma.phi[j] ~ dnorm(mu.phi.gamma, tau.phi.gamma)T(-5,5)
    gamma.pre[j] <- phi(gamma.phi[j])
    gamma[j] <- gamma.pre[j]*2

    
    #Loss aversion
    lambda.phi[j] ~ dnorm(mu.phi.lambda, tau.phi.lambda)T(-5,5)
    lambda.pre[j] <- phi(lambda.phi[j])
    lambda[j] <- lambda.pre[j]*5

    #Choice rule
    theta.phi[j] ~ dnorm(mu.phi.theta, tau.phi.theta)T(-5,5) 
    theta[j] <- phi(theta.phi[j])*10 # 5 # 5 is original

  }

    gamma.gain <- gamma
    gamma.loss <- gamma


  ####PRIORS FOR GROUP LEVEL
  
  #Value function
  mu.phi.alpha ~ dnorm(0,1)T(-5,5)
  sigma.phi.alpha ~ dunif(0,10)
  tau.phi.alpha <- pow(sigma.phi.alpha,-2)
  
  #Loss aversion
  mu.phi.lambda ~ dnorm(0,1)T(-5,5)
  sigma.phi.lambda ~ dunif(0,10) 
  tau.phi.lambda <- pow(sigma.phi.lambda,-2)
  
  #Probability weighting function
  mu.phi.gamma ~ dnorm(0,1)T(-5,5)
  sigma.phi.gamma ~ dunif(0,10)
  tau.phi.gamma <- pow(sigma.phi.gamma,-2)
  
  #Choice rule
  mu.phi.theta ~ dnorm(0,1)T(-5,5) 
  sigma.phi.theta ~ dunif(0,10)
  tau.phi.theta <- pow(sigma.phi.theta,-2)
  
  # To obtain the mean of the hyperdistribution on the desired scale
  mu.alpha    <- phi(mu.phi.alpha 		/ sqrt(1+ pow(sigma.phi.alpha, 2)))	*2
  mu.lambda   <- phi(mu.phi.lambda 		/ sqrt(1+ pow(sigma.phi.lambda,2)))	*5
  mu.gamma    <- phi(mu.phi.gamma  		/ sqrt(1+ pow(sigma.phi.gamma, 2)))	*2 
  mu.theta    <- phi(mu.phi.theta  		/ sqrt(1+ pow(sigma.phi.theta, 2)))	*5




  for (j in 1:nSubj) {  #Subject-loop
    #-------------------------
    #POSITIVE LOTTERY
    for (i in 1:nGain[j]) {  #Item loop

    
      #Value function
      #Lottery A
      v.x.a[i,j] <- pow(prospectsA[i,1,j],alpha[j])
      v.y.a[i,j] <- pow(prospectsA[i,3,j],alpha[j])
      v.z.a[i,j] <- pow(prospectsA[i,5,j],alpha[j])
      #Lottery B
      v.x.b[i,j] <- pow(prospectsB[i,1,j],alpha[j])       
      v.y.b[i,j] <- pow(prospectsB[i,3,j],alpha[j])
      v.z.b[i,j] <- pow(prospectsB[i,5,j],alpha[j])
      
      #Probability weighting function
      #Lottery A
      w.x.a[i,j] <- pow(cumprobsA[i,1,j],gamma[j]) / pow(pow(cumprobsA[i,1,j],gamma[j]) + pow((1-cumprobsA[i,1,j]),gamma[j]),(1/gamma[j])) 
      w.y.a[i,j] <- pow(cumprobsA[i,2,j],gamma[j])/  pow(pow(cumprobsA[i,2,j],gamma[j]) + pow((1-cumprobsA[i,2,j]),gamma[j]),(1/gamma[j])) - w.x.a[i,j]
      w.z.a[i,j] <- pow(cumprobsA[i,3,j],gamma[j])/  pow(pow(cumprobsA[i,3,j],gamma[j]) + pow((1-cumprobsA[i,3,j]),gamma[j]),(1/gamma[j])) - (w.y.a[i,j] + w.x.a[i,j])

      #Lottery B
      w.x.b[i,j] <- pow(cumprobsB[i,1,j],gamma[j]) / pow(pow(cumprobsB[i,1,j],gamma[j]) + pow((1-cumprobsB[i,1,j]),gamma[j]),(1/gamma[j])) 
      w.y.b[i,j] <- pow(cumprobsB[i,2,j],gamma[j])/  pow(pow(cumprobsB[i,2,j],gamma[j]) + pow((1-cumprobsB[i,2,j]),gamma[j]),(1/gamma[j])) - w.x.b[i,j]
      w.z.b[i,j] <- pow(cumprobsB[i,3,j],gamma[j])/  pow(pow(cumprobsB[i,3,j],gamma[j]) + pow((1-cumprobsB[i,3,j]),gamma[j]),(1/gamma[j])) - (w.y.b[i,j] + w.x.b[i,j])

      #Valuation 	
      Vf.a[i,j]  <- w.x.a[i,j] * v.x.a[i,j] + w.y.a[i,j] * v.y.a[i,j] + w.z.a[i,j] * v.z.a[i,j]
      Vf.b[i,j]  <- w.x.b[i,j] * v.x.b[i,j] + w.y.b[i,j] * v.y.b[i,j] + w.z.b[i,j] * v.z.b[i,j]
      
	
      #Choice rule
      binval[i,j] <- (1)/(1+exp((-1*theta[j])*(Vf.a[i,j]-Vf.b[i,j])))
      choices[i,j] ~ dbern(binval[i,j])
      choices.pred[i,j] ~ dbern(binval[i,j])
    }

		
    #-------------------------
    #NEGATIVE LOTTERY
    for (i in (nGain[j]+1):(nGain[j]+nLoss[j])) { # Item-Loop

      #Value function
      #Lottery A
      v.x.a[i,j] <- lambda[j]*(-1) * pow((abs(prospectsA[i,1,j])),alpha[j])       
      v.y.a[i,j] <- lambda[j]*(-1) * pow((abs(prospectsA[i,3,j])),alpha[j])
      v.z.a[i,j] <- lambda[j]*(-1) * pow((abs(prospectsA[i,5,j])),alpha[j])

      #Lottery B	
      v.x.b[i,j] <- lambda[j]*(-1) * pow((abs(prospectsB[i,1,j])),alpha[j])       
      v.y.b[i,j] <- lambda[j]*(-1) * pow((abs(prospectsB[i,3,j])),alpha[j]) 
      v.z.b[i,j] <- lambda[j]*(-1) * pow((abs(prospectsB[i,5,j])),alpha[j]) 


      #Probability weighting function
      #Lottery A
      w.x.a[i,j] <- pow(cumprobsA[i,3,j],gamma[j]) / pow(pow(cumprobsA[i,3,j],gamma[j]) + pow((1-cumprobsA[i,3,j]),gamma[j]),(1/gamma[j])) - pow(cumprobsA[i,4,j],gamma[j])/ pow(pow(cumprobsA[i,4,j],gamma[j]) + pow((1-cumprobsA[i,4,j]),gamma[j]),(1/gamma[j]))
      w.y.a[i,j] <- pow(cumprobsA[i,4,j],gamma[j]) / pow(pow(cumprobsA[i,4,j],gamma[j]) + pow((1-cumprobsA[i,4,j]),gamma[j]),(1/gamma[j])) - pow(cumprobsA[i,5,j],gamma[j])/ pow(pow(cumprobsA[i,5,j],gamma[j]) + pow((1-cumprobsA[i,5,j]),gamma[j]),(1/gamma[j]))
      w.z.a[i,j] <- pow(cumprobsA[i,5,j],gamma[j]) / pow(pow(cumprobsA[i,5,j],gamma[j]) + pow((1-cumprobsA[i,5,j]),gamma[j]),(1/gamma[j]))       

      #Lottery B
      w.x.b[i,j] <- pow(cumprobsB[i,3,j],gamma[j]) / pow(pow(cumprobsB[i,3,j],gamma[j]) + pow((1-cumprobsB[i,3,j]),gamma[j]),(1/gamma[j])) - pow(cumprobsB[i,4,j],gamma[j])/ pow(pow(cumprobsB[i,4,j],gamma[j]) + pow((1-cumprobsB[i,4,j]),gamma[j]),(1/gamma[j]))
      w.y.b[i,j] <- pow(cumprobsB[i,4,j],gamma[j]) / pow(pow(cumprobsB[i,4,j],gamma[j]) + pow((1-cumprobsB[i,4,j]),gamma[j]),(1/gamma[j])) - pow(cumprobsB[i,5,j],gamma[j])/ pow(pow(cumprobsB[i,5,j],gamma[j]) + pow((1-cumprobsB[i,5,j]),gamma[j]),(1/gamma[j]))
      w.z.b[i,j] <- pow(cumprobsB[i,5,j],gamma[j]) / pow(pow(cumprobsB[i,5,j],gamma[j]) + pow((1-cumprobsB[i,5,j]),gamma[j]),(1/gamma[j])) 

      #Valuation 	
      Vf.a[i,j]  <- w.x.a[i,j] * v.x.a[i,j] + w.y.a[i,j] * v.y.a[i,j] + w.z.a[i,j] * v.z.a[i,j]
      Vf.b[i,j]  <- w.x.b[i,j] * v.x.b[i,j] + w.y.b[i,j] * v.y.b[i,j] + w.z.b[i,j] * v.z.b[i,j]
   	
      #Choice rule
      binval[i,j] <- (1)/(1+exp((-1*theta[j])*(Vf.a[i,j]-Vf.b[i,j])))
      choices[i,j] ~ dbern(binval[i,j])
      choices.pred[i,j] ~ dbern(binval[i,j])
   }

   #-------------------------
    #MIXED LoTTERY Item-Loop
    for (i in (nGain[j]+nLoss[j]+1):(nGain[j]+nLoss[j]+nMixed[j])) { #Item loop


      #Value function
      #Lottery A
      v.x.a[i,j] <- pow(prospectsA[i,1,j],alpha[j])
      v.y.a[i,j] <- pow(prospectsA[i,3,j],alpha[j])
      v.z.a[i,j] <- (-1 * lambda[j]) * pow((abs(prospectsA[i,5,j])),alpha[j])   
    
      #Lottery B
      v.x.b[i,j] <- pow(prospectsB[i,1,j],alpha[j])  
      v.y.b[i,j] <- pow(prospectsB[i,3,j],alpha[j])          
      v.z.b[i,j] <- (-1 * lambda[j]) * pow((abs(prospectsB[i,5,j])),alpha[j])  

      #Weighting function
      #Lottery A
      w.x.a[i,j]  <- pow(cumprobsA[i,1,j],gamma[j]) / pow(pow(cumprobsA[i,1,j],gamma[j]) + pow((1-cumprobsA[i,1,j]),gamma[j]) ,(1/gamma[j])) 
      w.y.a[i,j]  <- pow(cumprobsA[i,2,j],gamma[j]) / pow(pow(cumprobsA[i,2,j],gamma[j]) + pow((1-cumprobsA[i,2,j]),gamma[j]) ,(1/gamma[j])) - w.x.a[i,j]
      w.z.a[i,j]  <- pow(cumprobsA[i,5,j],gamma[j]) / pow(pow(cumprobsA[i,5,j],gamma[j]) + pow((1-cumprobsA[i,5,j]),gamma[j]) ,(1/gamma[j]))

      #Lottery B
      w.x.b[i,j]  <- pow(cumprobsB[i,1,j],gamma[j]) / pow(pow(cumprobsB[i,1,j],gamma[j]) + pow((1-cumprobsB[i,1,j]),gamma[j]) ,(1/gamma[j])) 
      w.y.b[i,j]  <- pow(cumprobsB[i,2,j],gamma[j]) / pow(pow(cumprobsB[i,2,j],gamma[j]) + pow((1-cumprobsB[i,2,j]),gamma[j]) ,(1/gamma[j])) - w.x.b[i,j]
      w.z.b[i,j]  <- pow(cumprobsB[i,5,j],gamma[j]) / pow(pow(cumprobsB[i,5,j],gamma[j]) + pow((1-cumprobsB[i,5,j]),gamma[j]) ,(1/gamma[j]))
      
      #Valuation 	
      Vf.a[i,j]  <- w.x.a[i,j] * v.x.a[i,j] + w.y.a[i,j] * v.y.a[i,j] + w.z.a[i,j] * v.z.a[i,j]
      Vf.b[i,j]  <- w.x.b[i,j] * v.x.b[i,j] + w.y.b[i,j] * v.y.b[i,j] + w.z.b[i,j] * v.z.b[i,j]
      	
      #Choice rule     
      binval[i,j] <- (1)/(1+exp((-1*theta[j])*(Vf.a[i,j]-Vf.b[i,j])))
      choices[i,j] ~ dbern(binval[i,j])
      choices.pred[i,j] ~ dbern(binval[i,j])
    }
  }   
}

```

### The probability weighting function and other model changes

When going through the model code, you may have noted there are a few things different than before. For example, the domain-specific CPT code differs. One thing to note is that we now evaluate three possible outcomes in the model for each option instead of two, even though there were only 2 actual outcomes that could occur in the experiment. In the data, an artificial third outcome `0` with 0% probability was added post hoc. This structure with 3 potential outcomes makes our model more flexible to different paradigm implementations (sometimes you may want to use classical lotteries with two outcomes, sometimes more complex ones with three outcomes). 
Also, many dataframes and parameters now have different names (e.g., theta instead of rho). This shows how flexibly you can write your model. Basically, parameters and dataframes can have (almost) any name you want, but of course they should be clearly interpretable.

But more notably, the actual CPT functions differ: In our previous model, the probability weighting function looked like this (as introduced earlier):

```
      #Probability weighting function 
      #Lottery A
      pi.hA[i,j] <- (delta[j] * (pow(p_hA[i], gamma[j]))) / (delta[j] * (pow(p_hA[i], gamma[j])) + pow(p_lA[i], gamma[j]))
      pi.lA[i,j]<- 1 - pi.hA[i,j]
```

In contrast, now we are using a different probability weighting function that doesn't have a delta parameter anymore:

```
      #Probability weighting function
      #Lottery A
      w.x.a[i,j] <- pow(cumprobsA[i,1,j],gamma[j]) / pow(pow(cumprobsA[i,1,j],gamma[j]) + pow((1-cumprobsA[i,1,j]),gamma[j]),(1/gamma[j])) 
      w.y.a[i,j] <- pow(cumprobsA[i,2,j],gamma[j])/  pow(pow(cumprobsA[i,2,j],gamma[j]) + pow((1-cumprobsA[i,2,j]),gamma[j]),(1/gamma[j])) - w.x.a[i,j]
      w.z.a[i,j] <- pow(cumprobsA[i,3,j],gamma[j])/  pow(pow(cumprobsA[i,3,j],gamma[j]) + pow((1-cumprobsA[i,3,j]),gamma[j]),(1/gamma[j])) - (w.y.a[i,j] + w.x.a[i,j])
```

Why did we make that change? Whereas the general idea of CPT stays the same - assuming a subjective distortion of outcome and probability information - there have been several suggestions on how this distortion should be formalized mathematically. We will not go into detail about that here, but [Stott (2006)](https://link.springer.com/article/10.1007/s11166-006-8289-6) provides a nice overview.     

For now, just note that the weighting function we used in our first model `JAGS_cpt_model_individual.txt` is one of the most commonly used forms, it has two parameters (gamma and delta) and was proposed by Goldstein and Einhorn (1987).     
In our second model `JAGS_cpt_model_hierarchical_TK92-onedomain.txt`, we now want to fit the original weighting function proposed by Tversky and Kahneman (1992), which has only one parameter, the gamma parameter, but no elevation parameter delta.

Note also the data structure that the model expects: whereas our first model accessed cumulative probabilities fromm one-dimensional objects like `p_hA[i]`, now we use three-dimensional objects `cumprobsA[i,1,j]` that contain all cumulative probabilities from one option for all participants $j$ and all problems $i$.


### Hierarchical modeling

Another big difference is that we now implement a hierarchical modeling approach. What does this mean? Well, formal models can be applied to empirical data in several ways.    

First, one could estimate model parameters on the group-level - that is, the whole sample of participants — and analyze the data as if they were obtained from a single individual. This was done in the past when people were mainly interested in group means, and this approach thus disregards individual differences. This is very unlikely in real empirical data: Probably, individuals differ in how they behave.    

Second, one could take individual differences into account by estimating the model parameters separately for each participant. This is what we did before in our first model  `JAGS_cpt_model_individual.txt`, where we ended up with estimations for each parameter separately for every individual. Consequently, this approach assumes complete
independence.     

Both approaches have major drawbacks: Assuming no individual variation is empirically problematic, while estimating parameters for each individual participant independently may lead to noisy parameter estimates, given that in most experimental setups usually only a relatively small number of data points is available for each single participant.   

Hierarchical modeling offers a compromise between the above discussed approaches, as it allows to estimate both group-level and individual participant-level parameter estimates. For more details, please see [Nilsson et al. (2011)](https://doi.org/10.1016/j.jmp.2010.08.006). In short, this approach is characterized by assuming that individual parameters stem
from a group-level distribution.    

This hierarchical aspect is reflected in the following part of the model, where individual parameters are drawn from a normal distribution with mean `mu.phi.alpha` and a variance parameter `tau.phi.alpha`. Note that the variance parameter here is the *precision*, not SD or the classical variance. Precision is defined as 1/variance.

```
  #### PRIORS FOR INDIVIDUAL LEVEL
    #Value function 
    alpha.phi[j] ~ dnorm(mu.phi.alpha, tau.phi.alpha)T(-5,5)
    alpha.pre[j] <- phi(alpha.phi[j])
    alpha[j] <- alpha.pre[j]*2
```

What do `alpha.phi` and `alpha.pre` mean here? Remember when we used constrained beta distributions as uninformative priors in our previous non-hierarchical model:

```
    alpha.pre[j] ~ dbeta(1,1)
    alpha[j] <- (alpha.pre[j]) * 2
```
Here we are basically doing the same, `alpha[j]` is constrained between 0 and 2. However, instead of using a beta distribution, we first sample from the real line, and then use a phi-transformation to transform the sampled values to our parameter scale of interest. The fundamental idea is the same.

Ok. Now we know how individual parameters are drawn from the group distribution and how they are transformed to the parameter scale. But how do we set priors for the new group-level parameters? This is reflected in the following part of the model:

```
  ####PRIORS FOR GROUP LEVEL
  
  #Value function
  mu.phi.alpha ~ dnorm(0,1)T(-5,5)
  sigma.phi.alpha ~ dunif(0,10)
  tau.phi.alpha <- pow(sigma.phi.alpha,-2)
  
  #Loss aversion
  mu.phi.lambda ~ dnorm(0,1)T(-5,5)
  sigma.phi.lambda ~ dunif(0,10) 
  tau.phi.lambda <- pow(sigma.phi.lambda,-2)
  
  #Probability weighting function
  mu.phi.gamma ~ dnorm(0,1)T(-5,5)
  sigma.phi.gamma ~ dunif(0,10)
  tau.phi.gamma <- pow(sigma.phi.gamma,-2)
  
  #Choice rule
  mu.phi.theta ~ dnorm(0,1)T(-5,5) 
  sigma.phi.theta ~ dunif(0,10)
  tau.phi.theta <- pow(sigma.phi.theta,-2)
```
You see that setting priors for the group-level parameters works in a very similar way. For example, Group-level means are drawn from normal distributions with $M=0$ and $SD=1$, whereas Group-level variance parameters (e.g., `sigma.phi.alpha`) are drawn from uniform distributions. Group-level variance parameters are then transformed into precision values (e.g., `tau.phi.alpha`).     

Again, note that these parameters are sampled on the real line. To make them interpretable on our parameter scale, we have to apply a phi-transformation. When transforming the group-level mean, we also take the variance into account. Note also the muliplication factor at the end, which again ensures that group-level means lie in plausible ranges on the parameter scale.

```
  # To obtain the mean of the hyperdistribution on the desired scale
  mu.alpha    <- phi(mu.phi.alpha 		/ sqrt(1+ pow(sigma.phi.alpha, 2)))	*2
  mu.lambda   <- phi(mu.phi.lambda 		/ sqrt(1+ pow(sigma.phi.lambda,2)))	*5
  mu.gamma    <- phi(mu.phi.gamma  		/ sqrt(1+ pow(sigma.phi.gamma, 2)))	*2 
  mu.theta    <- phi(mu.phi.theta  		/ sqrt(1+ pow(sigma.phi.theta, 2)))	*5
```

To conclude: In hierarchical modeling, we can infer the group-level mean of a parameter through the estimated group-level mean as well as the degree of individual variability through the estimated group-level standard deviation. Additionally, we get individual parameter estimates for every subject. These individual parameter estimates are particularly robust in hierarchical models as they are constrained by the group-level distribution, allowing the estimation of individual parameters to benefit from the estimates of other participants. 


### Preparing the data for JAGS

Now that we have an understanding how the new hierarchical model is structured and how the data should be structured that it's trying to access, let's go on to prepare our data for the model.

```{r}
#| output: false
#| 
# read in choices
choices_exp <- read_xlsx(here("BayesianCognitiveModeling", "kellen2016.xlsx"), sheet = "Choices_experience_neu") %>% dplyr::rename(ProblemID = 1)
choices_long = choices_exp %>% gather(key = "ParticipantCode", value = "choice_num", -ProblemID) # gather choice data

exp_kellen2016 = merge(problems_exp, choices_long, by = c("ParticipantCode", "ProblemID")) %>%
  select(!ProblemID) %>%
  dplyr::select(code = ParticipantCode,
                A1o = outcomeA1, A1p = probA1,
                A2o = outcomeA2, A2p = probA2,
                A3o = outcomeA3, A3p = probA3,
                B1o = outcomeB1, B1p = probB1,
                B2o = outcomeB2, B2p = probB2,
                B3o = outcomeB3, B3p = probB3, # rename variables for more compact appearance
                choice_num,
                ProblemID = ProblemName) %>%
  arrange(code, ProblemID) %>% # arrange by sb and problem ID
  mutate(A1p = round(A1p, digits = 3),
         A2p = round(A2p, digits = 3), # round subjective probability values to 3 decimal places (to avoid issues with cumprob calculation) 
         B1p = round(B1p, digits = 3),
         B2p = round(B2p, digits = 3)) %>% 
  select(A1o:B3p, everything())  


# get overall n and gamble counts
subjects = unique(exp_kellen2016$code)
nSubjects =  length(subjects) # store count of subjects in this experiment for later use of variable
nGambles =  length(unique(exp_kellen2016$ProblemID)) # number of total gambles
```

Since sampling behavior differs between participants, the number of problems in each domain (Gain, Loss, Mixed) may also differ. Remember that we have to know the number of problems in each domain to loop over the specific sections in our JAGS model code. That's why it may be helpful to check and document the domain of the problems for each participant and to sort the problems accordingly.

```{r}

gambles_exp_sorted = exp_kellen2016 %>% 
  mutate(domain = case_when(A1o >= 0 & A2o >= 0 & A3o >= 0 & B1o >= 0 & B2o >= 0 & B3o >= 0   ~ 1, # "Gain" , 
                            A1o <= 0 & A2o <= 0 & A3o <= 0 & B1o <= 0 & B2o <= 0 & B3o <= 0 ~ 2, # "Loss" , 
                            .default = 3)) %>% # "Mixed"
  arrange(code, domain, ProblemID) %>%
  group_by(code) %>% dplyr::mutate(sort_key = row_number()) %>% ungroup() # create sort key to match the choice and problem objects accordingly later

# create list for domain count
domain_count <- vector("list", 4)
names(domain_count) <- c("nGambles", "nGain", "nLoss", "nMixed")

domain_count[["nGambles"]] = gambles_exp_sorted  %>% count(code, name = "n") %>% pull(n)
domain_count[["nGain"]] = gambles_exp_sorted  %>% filter(domain == 1) %>% count(code, name = "n") %>% pull(n)
domain_count[["nLoss"]]  = gambles_exp_sorted  %>% filter(domain == 2) %>% count(code, name = "n") %>% pull(n)
domain_count[["nMixed"]]  = gambles_exp_sorted %>% filter(domain == 3) %>% count(code, name = "n") %>% pull(n)
max_problems = max(domain_count[["nGambles"]])

```

Let's now re-transform the choice data to our standard format.

```{r}
# create df with choice data
choices_exp <- gambles_exp_sorted %>%
    dplyr::select(code, choice_num, sort_key) %>%
    pivot_wider(names_from = code, values_from = choice_num)  %>%
    select(-sort_key)

```

Then, we calculate the cumulative probabilities.

```{r}
# get only gamble columns
gambles = gambles_exp_sorted %>% select(A1o:B3p) %>% as.matrix()

#Order outcomes by size (this is necessary for the the calculating the cumulative probabilities, which are central in CPT)
gambles.ordered.A <- matrix(nrow=nrow(gambles),ncol = 6)
gambles.ordered.B <- matrix(nrow=nrow(gambles),ncol = 6)
for (i in 1:nrow(gambles)) {
  out <- sort(gambles[i,c(1,3,5)], index.return=TRUE,decreasing = T)
  gambles.ordered.A[i,] <- c(out$x[1], gambles[i,out$ix[1]*2], out$x[2], gambles[i,out$ix[2]*2], out$x[3], gambles[i,out$ix[3]*2])
  out <- sort(gambles[i,c(7,9,11)], index.return=TRUE,decreasing = T)
  gambles.ordered.B[i,] <- c(out$x[1], gambles[i,out$ix[1]*2+6], out$x[2], gambles[i,out$ix[2]*2+6], out$x[3], gambles[i,out$ix[3]*2+6])  
}

gambles.ordered.A <- gambles.ordered.A %>% as.data.frame() %>% `colnames<-`(c("A1o", "A1p", "A2o", "A2p", "A3o", "A3p")) # rename columns
gambles.ordered.B <- gambles.ordered.B %>% as.data.frame() %>% `colnames<-`(c("B1o", "B1p", "B2o", "B2p", "B3o", "B3p"))
gambles <- cbind(gambles.ordered.A,gambles.ordered.B) # bind together

# merge outcome-ordered gamble info with rest of dataframe
gambles = gambles_exp_sorted %>% select(-c(A1o:B3p)) %>% cbind(as.data.frame(gambles)) 

#Calculate cumulative probabilities
cumprobs = gambles %>% select(A1o:B3p) %>% as.matrix()
cumprobsA = cbind(cumprobs[,2], rowSums(cumprobs[,c(2, 4)]), rowSums(cumprobs[,c(2, 4, 6)]), rowSums(cumprobs[,c(4, 6)]), cumprobs[,6])  %>% 
  as.data.frame() %>% `colnames<-`(c("CPA1", "CPA2", "CPA3", "CPA4", "CPA5" ))
cumprobsB = cbind(cumprobs[,8], rowSums(cumprobs[,c(8, 10)]), rowSums(cumprobs[,c(8, 10, 12)]), rowSums(cumprobs[,c(10, 12)]), cumprobs[,12]) %>%
  as.data.frame() %>% `colnames<-`(c("CPB1", "CPB2", "CPB3", "CPB4", "CPB5" ))
cumprobs <- cbind(cumprobsA,cumprobsB) # bind together
gambles <- cbind(gambles,cumprobs) # also with overall df
```

Lastly, we reformat all problem data in 3D objects (tensors) for our JAGS model.

```{r}
# for prospects
prospectsA.3d <- array(NA, dim = c(max_problems,  ncol(select(gambles, A1o:A3p)),  length(subjects)))
for (j in seq_along(subjects)) {
  subject_data <- gambles %>% filter(code == subjects[j]) %>% select(A1o:A3p)
  prospectsA.3d[1:nrow(subject_data), , j] <- as.matrix(subject_data)
}

prospectsB.3d <- array(NA, dim = c(max_problems,  ncol(select(gambles, B1o:B3p)),  length(subjects)))
for (j in seq_along(subjects)) {
  subject_data <- gambles %>% filter(code == subjects[j]) %>% select(B1o:B3p)
  prospectsB.3d[1:nrow(subject_data), , j] <- as.matrix(subject_data)
}

# for cumulative probabilities
cumprobsA.3d <- array(NA, dim = c(max_problems, ncol(select(gambles, CPA1:CPA5)), length(subjects)))
for (j in seq_along(subjects)) {
  subject_data <- gambles %>% filter(code == subjects[j]) %>% select(CPA1:CPA5)
  cumprobsA.3d[1:nrow(subject_data), , j] <- as.matrix(subject_data)
}

cumprobsB.3d <- array(NA, dim = c(max_problems, ncol(select(gambles, CPB1:CPB5)), length(subjects)))
for (j in seq_along(subjects)) {
  subject_data <- gambles %>% filter(code == subjects[j]) %>% select(CPB1:CPB5)
  cumprobsB.3d[1:nrow(subject_data), , j] <- as.matrix(subject_data)
}
```


Finally, we proceed with preparing the final data object, listing the parameters to be monitored, and set initial values for sampling.

```{r}
# collected data for JAGS
 data <- list(choices = as.matrix(choices_exp),
              nSubj = nSubjects,
              prospectsA = prospectsA.3d ,
              prospectsB = prospectsB.3d,
              nGain = domain_count[["nGain"]],
              nLoss = domain_count[["nLoss"]],
              nMixed = domain_count[["nMixed"]],
              cumprobsA = cumprobsA.3d,
              cumprobsB = cumprobsB.3d)

#Determine the parameters to be monitored in JAGS	
parameters <- c("alpha","lambda", "theta", "gamma", # individual parameters
                "mu.alpha","mu.lambda", "mu.theta", "mu.gamma")  # group-level parameters

#Set initial values of the parameters
oneinits <- list(alpha.phi = rep(qnorm(.4),nSubjects), 
                 gamma.phi = rep(qnorm(.5),nSubjects), 
                 lambda.phi = rep(qnorm(.5),nSubjects), 
                 theta.phi = rep(qnorm(.01),nSubjects), 
                 mu.phi.alpha = qnorm(.4), 
                 mu.phi.gamma = qnorm(.5), 
                 mu.phi.lambda= qnorm(.5), 
                 mu.phi.theta = qnorm(.01))
myinits <- list(oneinits)
```

And then we run our hierarchical model through JAGS. 

```{r}
# if RDS file exists, load existing fits 
if(file.exists(here("BayesianCognitiveModeling", "modelfits", "cpt_model_hierarchical.rds"))){
  samples.exp <- readRDS(file = here("BayesianCognitiveModeling", "modelfits", "cpt_model_hierarchical.rds")) 
  print(paste("model loaded"))
  
# if RDS file doesn't exist, rerun fits:
} else {

#Run JAGS using R2jags
samples.exp <- jags.parallel(data = data, # empirical data
                             inits = myinits, # initial values for parameters in each chain
                             parameters = parameters, # parameters we want to record
                             model.file = here("BayesianCognitiveModeling", "JAGS_models", "JAGS_cpt_model_hierarchical_TK92-onedomain.txt"), # model file
                             n.chains = 8, # number of chains 
                             n.iter = 15000, # number of iterations per chain
                             n.burnin = 5000, # burn-in period
                             n.thin = 16, # thinning to reduce autocorrelation during sampling process
                             n.cluster = 8, # number of cores to use: compute MCMC chains in parallel
                             jags.seed = 2024,
                             DIC=T)

saveRDS(samples.exp, here("BayesianCognitiveModeling", "modelfits", "cpt_model_hierarchical.rds")) # once the model is done fitting, we first save the samples to make sure they don't get lost.
print(paste("done fitting model to data"))
}

```

Looking at the new JAGS object, we see that we not only have individual parameter estimates now:

```{r}
kable(head(samples.exp$BUGSoutput$summary)) 
```

But also group-level parameter estimates:

```{r}
samples.exp$BUGSoutput$summary %>%
    as.data.frame() %>%
    rownames_to_column("parameter") %>%
    filter(str_detect(parameter, "mu")) %>%
    kable()
```

Using our plotting commands, you can now plot the individual and group-level value function at once:

```{r value-function-hierarchical}
v_fun_TK92(samples = samples.exp, color = "red")
```

And similarly for the probability weighting function. 

```{r weighting-function-hierarchical}
w_fun_TK92(samples = samples.exp, color = "red")
```

# Debugging JAGS errors

As always in everyday life when working with code, it is likely that you will run into errors at some point. For JAGS, unfortunately, the error messages are often not very informative.
While there is an unpredictable number of errors that JAGS can throw out, here, we treat the below list as a work-in-progress documentation of common errors that we have encountered, and what might be potential ways to solve them.

We will continue expanding this section.
